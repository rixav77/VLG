#  **VLG Dataset Image Classification Project**

## ğŸŒŸ **Overview**
ğŸ—‚ï¸ This repository contains resources and code for classifying images into 50 ğŸ“š categories using ğŸ‹ï¸â€â™‚ï¸ ResNet50. It details steps such as data preprocessing, model fine-tuning, training, and ğŸ“ˆ making predictions, providing a robust framework for image classifications.

---

## ğŸ“ **Repository Structure**

- ğŸ“ **`notebook.ipynb`**: ğŸ“˜ Jupyter notebook with the complete ğŸš€ implementation pipeline.
- ğŸ’¾ **`best_model.pth`**: ğŸ”’ Checkpoint for the trained ResNet50 model.
- ğŸ“Š **`predictions.csv`**: ğŸ“‘ File containing submission-ready ğŸ“ predictions for the test dataset.
- ğŸ“„ **`README.md`**: ğŸ“œ File explaining ğŸ“š project details and instructions.

---

## âš™ï¸ **Prerequisites**

ğŸ”§ Install these ğŸ› ï¸ tools to set up the environment:

- ğŸ Python 
- ğŸ”¥ PyTorch 
- ğŸ“¸ torchvision 
- ğŸ“ scikit-learn
- ğŸ¨ matplotlib
- ğŸ“Š pandas
- ğŸ““ Jupyter Notebook

---

## ğŸ—‚ï¸ **Datasets**

ğŸ“‚ The dataset is divided into ğŸ‹ï¸â€â™‚ï¸ training, ğŸ“Š validation, and ğŸ“ test sets. It facilitates essential tasks:

- ğŸ“¥ Loading and preprocessing ğŸ–¼ï¸ data.
- ğŸ”€ Splitting data into training and validation subsets.
- ğŸ› ï¸ Generating predictions for unseen test data.

---

## ğŸ”‘ **Key Steps in Implementation**

### 1ï¸âƒ£ **ğŸ“¥ Data Loading & Preprocessing**

- **ğŸ” Description**: The dataset is read, resized to 224x224 pixels, normalized for ResNet50, and converted into tensor format. Each image undergoes transformation to maintain consistency with ResNet50 input requirements.
- **ğŸ§‘â€ğŸ’» Code Highlights**:
  ```python
  transforms.Compose([
      transforms.Resize((224, 224)),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
  ])
  ```
- **Rationale**: These transformations ensure pixel intensities are scaled, and the model input format is maintained.

### 2ï¸âƒ£ **ğŸ› ï¸ Model Fine-Tuning**

- **ğŸ” Description**: The ResNet50 architecture is modified to support 50 ğŸ·ï¸ output categories by adding a custom fully connected layer. A dropout layer is included to reduce overfitting.
- **ğŸ§‘â€ğŸ’» Code Highlights**:
  ```python
  model.fc = nn.Sequential(
      nn.Linear(num_features, 512),
      nn.ReLU(),
      nn.Dropout(p=0.5),
      nn.Linear(512, 50)
  )
  ```
- **Rationale**: Transfer learning leverages pretrained weights, while the fully connected layer adapts the model to our specific classification task.

### 3ï¸âƒ£ **ğŸ‹ï¸â€â™‚ï¸ Training & Validation**

- **ğŸ” Description**: The model is trained using the Adam optimizer with weight decay for regularization, and a StepLR scheduler dynamically adjusts the learning rate to enhance convergence.
- **ğŸ§‘â€ğŸ’» Code Highlights**:
  ```python
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
  torch.save(model.state_dict(), "best_model.pth")
  ```
- **Rationale**: The combination of optimization techniques ensures efficient training and prevents overfitting.

### 4ï¸âƒ£ **ğŸ” Evaluation**

- **ğŸ” Description**: The model is evaluated on validation data using metrics such as accuracy and loss, with trends plotted to monitor training effectiveness.
- **Key Metrics**: Validation accuracy of ~98.01% highlights the modelâ€™s strong generalization.

### 5ï¸âƒ£ **ğŸ“¤ Prediction & Submission**

- **ğŸ” Description**: Predictions are generated for the test dataset, and results are formatted into a CSV file for submission.
- **ğŸ§‘â€ğŸ’» Code Highlights**:
  ```python
  test_preds = []
  for images in test_loader:
      outputs = model(images.to(device))
      _, preds = torch.max(outputs, 1)
      test_preds.extend(preds.cpu().numpy())
  ```
- **Rationale**: This process ensures compatibility with competition requirements.

---

## ğŸ† **Results**

- **ğŸ¯ Training Accuracy**: Achieved an impressive ğŸ“ˆ ~95.63%.
- **ğŸ¯ Validation Accuracy**: Reached ğŸ“Š ~98.01%, demonstrating robust model generalization.



## ğŸŒŸ **Vital Components**

1ï¸âƒ£ **ğŸ‹ï¸â€â™‚ï¸ Pretrained ResNet50**: Utilizes transfer learning for high ğŸ¯ accuracy.
2ï¸âƒ£ **ğŸ§  Custom Fully Connected Layer**: Adapts ResNet50 for the 50-category ğŸ·ï¸ classification task.
3ï¸âƒ£ **ğŸšï¸ Learning Rate Scheduler**: Dynamically adjusts learning rates for efficient training.
4ï¸âƒ£ **â›” Dropout Layers**: Reduces ğŸ“‰ overfitting, ensuring better generalization on unseen datasets.
5ï¸âƒ£ **ğŸ“¥ Data Preprocessing**: Rescales and normalizes images to align with model input requirements.
6ï¸âƒ£ **ğŸ› ï¸ Custom Training Loop**: Integrates evaluation and scheduler updates within each epoch for streamlined performance monitoring.

---

## ğŸ’¬ **Comments & Documentation**

ğŸ’¡ The code is extensively commented to provide clear explanations for:

- ğŸ”§ Data preprocessing steps.
- ğŸ› ï¸ Model ğŸ›ï¸ architecture modifications.
- ğŸ”„ Training and validation loops.
- ğŸ“Š Test data prediction and result generation.

---

## ğŸŒŸ **Conclusion**
This project showcases a robust approach to image classification using a ResNet50 backbone, custom fully connected layer, and a learning rate scheduler. It provides a solid foundation for further exploration and fine-tuning for optimal performance in image classification tasks.

